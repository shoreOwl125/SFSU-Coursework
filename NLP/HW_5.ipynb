{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kAo_dSBsc1oL"
      },
      "outputs": [],
      "source": [
        "# CSC 820 Homework 5\n",
        "# Andrew Dahlstrom\n",
        "# 3/6/2024\n",
        "\n",
        "#!pip install -U pip\n",
        "#!pip install -U dill\n",
        "#!pip install -U nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "DozJuDk2c7QP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4c0e80-c3d1-4386-b85b-077ec2d0be28"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First download the file of Trump's tweets from Kaggle and store\n",
        "# it in a virtual drive or locally, it can be found here:\n",
        "# https://www.kaggle.com/datasets/austinreese/trump-tweets?select=realdonaldtrump.csv\n",
        "\n",
        "# Unzip the file from mounted Google drive\n",
        "zip_file = '/content/drive/MyDrive/Colab Notebooks/CSC 820/realdonaldtrump.csv.zip'\n",
        "\n",
        "with ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extract('realdonaldtrump.csv')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('realdonaldtrump.csv')\n",
        "\n",
        "# Tokenize the text content\n",
        "trump_corpus = list(df['content'].apply(word_tokenize))"
      ],
      "metadata": {
        "id": "xBZ2aezoc-iS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the tokenized text for trigrams\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, trump_corpus)"
      ],
      "metadata": {
        "id": "eZJbtFPCdAzJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Maximum Likelihood Estimation model with the training data by\n",
        "# converting the trigrams into frequency distributions, where the model counts\n",
        "# the occurrences of each n-gram in the training data\n",
        "trump_model = MLE(n)\n",
        "trump_model.fit(train_data, padded_sents)"
      ],
      "metadata": {
        "id": "1WICVXrhdEGY"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This detokenizer is used to reverse the tokenization process,\n",
        "# converting a sequence of tokens back into a single string representing a sentence\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "# This method is implemented from a numpyninja post by Namrata Kapoor\n",
        "# https://www.numpyninja.com/post/n-gram-and-its-use-in-text-generation\n",
        "# It is used to generate a sentence using the trigram MLE model\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"\n",
        "    Generate a sentence using the given ngram language model.\n",
        "\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Maximum number of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    :return: Generated sentence as a string.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ],
      "metadata": {
        "id": "8UhKRT6mdF06"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a few sentences using the trained language model and adjusting the random seed\n",
        "for seed in range(33, 36):\n",
        "    generated_sentence = generate_sent(trump_model, num_words=20, random_seed=seed)\n",
        "    print(generated_sentence)"
      ],
      "metadata": {
        "id": "T9G7fm9zdG4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93970749-e326-4333-fc24-46283a6c7669"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "beat him on T.V . shows talking negatively about my management style . I work in exposing the evils of\n",
            "aircraft carrier stolen from us as we give our Country . But by the migrants allowed to be back on\n",
            "another phony story that I â€™ m sending out for free . You have to fight ugly wind turbines.\n"
          ]
        }
      ]
    }
  ]
}