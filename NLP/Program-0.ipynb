{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1611714,
          "sourceType": "datasetVersion",
          "datasetId": 951515
        }
      ],
      "dockerImageVersionId": 30019,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Mining : TF-IDF and Cosine Similarity from Scratch\n",
        "\n",
        "Table of Contents:\n",
        "\n",
        "1. Term Frequency (TF)\n",
        "2. Inverse Document Frequency (IDF)\n",
        "3. TF * IDF\n",
        "4. Vector Space Models and Representation – Cosine Similarity\n",
        "\n",
        "*** Any feedback or feature requests are welcome!***\n"
      ],
      "metadata": {
        "id": "pZNNI2R3k042"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us imagine that you are doing a search on below documents with the following query: **life learning**\n",
        "\n",
        "* **Document 1** : I want to start learning to charge something in life.\n",
        "* **Document 2** : learning something about me no one else knows\n",
        "* **Document 3** : Never stop learning\n",
        "\n",
        "The query is a free text query. It means a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators.\n",
        "\n",
        "Let us go over each step in detail to see how it all works."
      ],
      "metadata": {
        "id": "i1P6L0yVk045"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Term Frequency(TF)\n",
        "Term Frequency also known as TF measures the number of times a term (word) occurs in a document. Given below is the code and the terms and their frequency on each of the document.\n"
      ],
      "metadata": {
        "id": "htRoVidqk046"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "id": "J6dumzEFk046"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#documents\n",
        "doc1 = \"I want to start learning to charge something in life\"\n",
        "doc2 = \"reading something about life no one else knows\"\n",
        "doc3 = \"Never stop learning\"\n",
        "#query string\n",
        "query = \"life learning\""
      ],
      "metadata": {
        "trusted": true,
        "id": "I4NoJkodk047"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE :** Text Preprocessing Steps are ignored as the objective of this kernel is to explain and develop TF-IDF and cosine similarity from scratch"
      ],
      "metadata": {
        "id": "nCdDWPRNk047"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#term -frequenvy :word occurences in a document\n",
        "def compute_tf(docs_list):\n",
        "    # Iterate over each document\n",
        "    for doc in docs_list:\n",
        "        # Split the document into a list of words\n",
        "        doc1_lst = doc.split(\" \")\n",
        "        # Create a dictionary with each unique word as keys and their counts to 0\n",
        "        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n",
        "        # Count the occurrences of each word in document\n",
        "        for token in doc1_lst:\n",
        "            # Increment count for each word occurrence\n",
        "            wordDict_1[token] +=  1\n",
        "        # Create a DataFrame from the dictionary where of words and their frequencies\n",
        "        df = pd.DataFrame([wordDict_1])\n",
        "        # Insert a new column at the first position\n",
        "        idx = 0\n",
        "        # This will label the rows as 'Term Frequency'\n",
        "        new_col = [\"Term Frequency\"]\n",
        "        df.insert(loc=idx, column='Document', value=new_col)\n",
        "        print(df)\n",
        "\n",
        "compute_tf([doc1, doc2, doc3])"
      ],
      "metadata": {
        "trusted": true,
        "id": "POOkX-oSk048"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In reality each document will be of different size. On a large document the frequency of the terms will be much higher than the smaller ones. Hence we need to normalize the document based on its size.\n",
        "* A simple trick is to divide the term frequency by the total number of terms.\n",
        "* For example in Document 1 the term game occurs two times. The total number of terms in the document is 10. Hence the normalized term frequency is 2 / 10 = 0.2.\n",
        "\n",
        "\n",
        "Given below are the normalized term frequency for all the documents."
      ],
      "metadata": {
        "id": "QbvCqm7hk048"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalized Term Frequency\n",
        "def termFrequency(term, document):\n",
        "    # Convert the document to lowercase and split into words\n",
        "    normalizeDocument = document.lower().split()\n",
        "    # Calculate the term frequency as number of occurrences of the term divided by total number of terms in document\n",
        "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
        "\n",
        "def compute_normalizedtf(documents):\n",
        "    # Initialize an empty list to store the term frequency dictionaries\n",
        "    tf_doc = []\n",
        "    # Iterate over each document in list of documents\n",
        "    for txt in documents:\n",
        "        # Split the document text into words\n",
        "        sentence = txt.split()\n",
        "        # Create a dictionary to keep track of normalized term frequency for unique words in the document\n",
        "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
        "        # Calculate the normalized term frequency for each word and update the dictionary\n",
        "        for word in sentence:\n",
        "            norm_tf[word] = termFrequency(word, txt)\n",
        "        # Append the dictionary to the list\n",
        "        tf_doc.append(norm_tf)\n",
        "        df = pd.DataFrame([norm_tf])\n",
        "        idx = 0\n",
        "        new_col = [\"Normalized TF\"]\n",
        "        df.insert(loc=idx, column='Document', value=new_col)\n",
        "        print(df)\n",
        "    # Return the list of dictionaries containing the normalized term frequencies of each document\n",
        "    return tf_doc\n",
        "\n",
        "tf_doc = compute_normalizedtf([doc1, doc2, doc3])"
      ],
      "metadata": {
        "trusted": true,
        "id": "7TTSUK5bk048"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Inverse Document Frequency (IDF)\n",
        "\n",
        "* The main purpose of doing a search is to find out relevant documents matching the query.\n",
        "* In Term Frequecy all terms are considered equally important. In fact certain terms that occur too frequently have little power in determining the relevance.\n",
        "* We need a way to weigh down the effects of too frequently occurring terms. Also the terms that occur less in the document can be more relevant.\n",
        "* We need a way to weigh up the effects of less frequently occurring terms. Logarithms helps us to solve this problem.Logarithms helps us to solve this problem.\n",
        "\n",
        "\n",
        "Let us compute IDF for the term start\n",
        "\n",
        "IDF(start) = 1 + loge(Total Number Of Documents / Number Of Documents with term start in it)\n",
        "\n",
        "There are 3 documents in all = Document1, Document2, Document3\n",
        "The term start appears in Document1\n",
        "\n",
        " IDF(start) = 1 + loge(3 / 1)\n",
        "            = 1 + 1.098726209\n",
        "            = 2.098726209"
      ],
      "metadata": {
        "id": "OGL-So2Zk048"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inverseDocumentFrequency(term, allDocuments):\n",
        "    # Initialize a counter for number of documents containing term\n",
        "    numDocumentsWithThisTerm = 0\n",
        "    # Loop through each document in the list to check if it contains the term\n",
        "    for doc in range (0, len(allDocuments)):\n",
        "        # Convert the document to lowercase, split into words, check if the term is in document\n",
        "        if term.lower() in allDocuments[doc].lower().split():\n",
        "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
        "    # Calculate the IDF using the logarithm scale formula if term is in any document\n",
        "    if numDocumentsWithThisTerm > 0:\n",
        "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
        "    else:\n",
        "        # Return 1.0 if the term is not found in any document\n",
        "        return 1.0\n",
        "\n",
        "def compute_idf(documents):\n",
        "    idf_dict = {}\n",
        "    for doc in documents:\n",
        "        # Split the document into words\n",
        "        sentence = doc.split()\n",
        "        # Compute IDF for each word and update dict\n",
        "        for word in sentence:\n",
        "            idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
        "    return idf_dict\n",
        "idf_dict = compute_idf([doc1, doc2, doc3])\n",
        "\n",
        "compute_idf([doc1, doc2, doc3])"
      ],
      "metadata": {
        "trusted": true,
        "id": "EH_mNvkSk049"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.TF * IDF"
      ],
      "metadata": {
        "id": "oIIvN5Tvk049"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember we are trying to find out relevant documents for the query: **life learning**\n",
        "\n",
        "* For each term in the query multiply its normalized term frequency with its IDF on each document.\n",
        "* In Document1 for the term life the normalized term frequency is 0.1 and its IDF is 1.405465108.\n",
        "* Multiplying them together we get 0.140550715 (0.1 * 1.405465108).\n",
        "*\n",
        "Given below is TF * IDF calculations for life and learning in all the documents."
      ],
      "metadata": {
        "id": "8iugipW7k04-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf score across all docs for the query string(\"life learning\")\n",
        "def compute_tfidf_with_alldocs(documents , query):\n",
        "    tf_idf = []\n",
        "    index = 0\n",
        "    # Split the query into individual words\n",
        "    query_tokens = query.split()\n",
        "    # Create DataFrame with column for each query token and index column for documents\n",
        "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
        "    # Iterate over each document in documents list\n",
        "    for doc in documents:\n",
        "        df['doc'] = np.arange(0 , len(documents))\n",
        "        # Retrieve normalized term frequency dictionary for document\n",
        "        doc_num = tf_doc[index]\n",
        "        sentence = doc.split()\n",
        "        # Loop through each word in document\n",
        "        for word in sentence:\n",
        "            for text in query_tokens:\n",
        "                if(text == word):\n",
        "                    idx = sentence.index(word)\n",
        "                    # Calculate the TF-IDF score by multiplying term frequency by inverse document frequency\n",
        "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
        "                    # Store TF-IDF score in DataFrame\n",
        "                    tf_idf.append(tf_idf_score)\n",
        "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
        "        index += 1\n",
        "    # Fill missing values with 0 for terms that do not appear in document\n",
        "    df.fillna(0 , axis=1, inplace=True)\n",
        "    # Return list of TF-IDF scores and DataFrame containing scores for document and term\n",
        "    return tf_idf , df\n",
        "\n",
        "documents = [doc1, doc2, doc3]\n",
        "tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n",
        "print(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dvTJhTZQk04-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Vector Space Models and Representation  – Cosine Similarity\n",
        "\n",
        "The set of documents in a collection then is viewed as a set of vectors in a vector space. Each term will have its own axis. Using the formula given below we can find out the similarity between any two documents.\n",
        "\n",
        "* > Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
        "* > Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * … * d1[n] * d2[n]\n",
        "* > ||d1|| = square root(d1[0]2 + d1[1]2 + ... + d1[n]2)\n",
        "* > ||d2|| = square root(d2[0]2 + d2[1]2 + ... + d2[n]2)\n"
      ],
      "metadata": {
        "id": "K0UpVuOAk04-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"../input/tfidf-kernel/cosinesimilarity.jpg\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "tMHVGYVck04_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vectors deals only with numbers. In this example we are dealing with text documents. This was the reason why we used TF and IDF to convert text into numbers so that it can be represented by a vecto\n",
        "\n",
        "\n",
        "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query"
      ],
      "metadata": {
        "id": "xAAvrAOXk04_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalized TF for the query string(\"life learning\")\n",
        "def compute_query_tf(query):\n",
        "    query_norm_tf = {}\n",
        "    tokens = query.split()\n",
        "    for word in tokens:\n",
        "        query_norm_tf[word] = termFrequency(word , query)\n",
        "    return query_norm_tf\n",
        "query_norm_tf = compute_query_tf(query)\n",
        "print(query_norm_tf)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-n7rp1Qyk04_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#idf score for the query string(\"life learning\")\n",
        "def compute_query_idf(query):\n",
        "    idf_dict_qry = {}\n",
        "    # Split the query into individual words\n",
        "    sentence = query.split()\n",
        "    documents = [doc1, doc2, doc3]\n",
        "    # Loop over each token in query\n",
        "    for word in sentence:\n",
        "        # Calculate normalized term frequency of word\n",
        "        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n",
        "    # Return dictionary containing normalized term frequencies\n",
        "    return idf_dict_qry\n",
        "idf_dict_qry = compute_query_idf(query)\n",
        "print(idf_dict_qry)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mxRGg4sKk04_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf score for the query string(\"life learning\")\n",
        "def compute_query_tfidf(query):\n",
        "    tfidf_dict_qry = {}\n",
        "    sentence = query.split()\n",
        "    # Loop over each token in query\n",
        "    for word in sentence:\n",
        "        # Calculate the TF-IDF score for each word by multiplying its normalized term\n",
        "        # frequency (TF) by its inverse document frequency (IDF)\n",
        "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
        "    return tfidf_dict_qry\n",
        "tfidf_dict_qry = compute_query_tfidf(query)\n",
        "print(tfidf_dict_qry)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1UuYspb2k05A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now calculate the cosine similarity of the query and Document1.\n",
        "\n",
        "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
        "\n",
        "Dot product(Query, Document1)\n",
        "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
        "     = 0.197545035151\n",
        "\n",
        "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
        "\n",
        "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
        "\n",
        "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
        "                                        = 0.197545035151 / 0.197545035151\n",
        "                                        = 1"
      ],
      "metadata": {
        "id": "eSIFSlddk05A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
        "\n",
        "\"\"\"\n",
        "Example : Dot roduct(Query, Document1)\n",
        "\n",
        "     life:\n",
        "     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  /\n",
        "     sqrt(tfidf(life w.r.t query)) *\n",
        "     sqrt(tfidf(life w.r.t doc1))\n",
        "\n",
        "     learning:\n",
        "     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n",
        "     sqrt(tfidf(learning w.r.t query)) *\n",
        "     sqrt(tfidf(learning w.r.t doc1))\n",
        "\n",
        "\"\"\"\n",
        "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n",
        "    # Initialize variables to calculate dot product and magnitudes\n",
        "    dot_product = 0\n",
        "    qry_mod = 0\n",
        "    doc_mod = 0\n",
        "    # Split the query into individual words\n",
        "    tokens = query.split()\n",
        "    # Calculate dot product and magnitudes for cosine similarity calculation\n",
        "    for keyword in tokens:\n",
        "        # Increment dot product by product of TF-IDF scores\n",
        "        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n",
        "        #||Query||\n",
        "        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n",
        "        #||Document||\n",
        "        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n",
        "    # Take the square root of the Euclidean norm of query and document\n",
        "    qry_mod = np.sqrt(qry_mod)\n",
        "    doc_mod = np.sqrt(doc_mod)\n",
        "    #implement formula\n",
        "    denominator = qry_mod * doc_mod\n",
        "    # Calculate cosine similarity as ratio of dot product to magnitudes\n",
        "    cos_sim = dot_product/denominator\n",
        "\n",
        "    return cos_sim\n",
        "\n",
        "from collections import Iterable\n",
        "def flatten(lis):\n",
        "     for item in lis:\n",
        "        # Check if item is iterable and not a string, to avoid splitting strings into characters\n",
        "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
        "             # If the item is an iterable recursively flatten\n",
        "             for x in flatten(item):\n",
        "                yield x\n",
        "        else:\n",
        "             # If not yield it directly\n",
        "             yield item\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "11sMS6lIk05A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_similarity_docs(data):\n",
        "    cos_sim =[]\n",
        "    for doc_num in range(0 , len(data)):\n",
        "        # Calculate cosine similarity for current document and query\n",
        "        # append the result to cos_sim list\n",
        "        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n",
        "    # Return list of cosine similarity scores\n",
        "    return cos_sim\n",
        "similarity_docs = rank_similarity_docs(documents)\n",
        "doc_names = [\"Document1\", \"Document2\", \"Document3\"]\n",
        "print(doc_names)\n",
        "print(list(flatten(similarity_docs)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "PC1mcamAk05B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I plotted vector values for the query and documents in 2-dimensional space of life and learning. Document1 has the highest score of 1. This is not surprising as it has both the terms life and learning."
      ],
      "metadata": {
        "id": "9zyLULMwk05B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(\"../input/tfidf-kernel/cosinesimiarlity11.jpeg\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Fyk1rxZVk05B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}