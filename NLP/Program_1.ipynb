{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text Mining : TF-IDF and Cosine Similarity from Scratch\n",
        "\n",
        "Table of Contents:\n",
        "\n",
        "1. Term Frequency (TF)\n",
        "2. Inverse Document Frequency (IDF)\n",
        "3. TF * IDF\n",
        "4. Vector Space Models and Representation â€“ Cosine Similarity\n",
        "\n",
        "*** Any feedback or feature requests are welcome!***\n"
      ],
      "metadata": {
        "id": "g4XhTSCWdq80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us imagine that you are doing a search on below documents with the following query: **life learning**\n",
        "\n",
        "* **Document 1** : I want to start learning to charge something in life.\n",
        "* **Document 2** : learning something about me no one else knows\n",
        "* **Document 3** : Never stop learning\n",
        "\n",
        "The query is a free text query. It means a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators.\n",
        "\n",
        "Let us go over each step in detail to see how it all works."
      ],
      "metadata": {
        "id": "naaZBEo2dq83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Term Frequency(TF)\n",
        "Term Frequency also known as TF measures the number of times a term (word) occurs in a document. Given below is the code and the terms and their frequency on each of the document.\n"
      ],
      "metadata": {
        "id": "98phUS03dq83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "trusted": true,
        "id": "7XmhmLICdq84"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/CSC 820/transcripts.csv'\n",
        "data = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "nEwxlRo9625E"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust sample size depending on amount of time you have to run program\n",
        "reduced_data = data.sample(frac=0.01, random_state=42)\n",
        "documents = reduced_data['transcript'].tolist()\n",
        "# Query strings\n",
        "query1 = \"life learning\"\n",
        "query2 = \"hard work\"\n",
        "query3 = \"self actualization\""
      ],
      "metadata": {
        "trusted": true,
        "id": "I1kGRagydq85"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE :** Text Preprocessing Steps are ignored as the objective of this kernel is to explain and develop TF-IDF and cosine similarity from scratch"
      ],
      "metadata": {
        "id": "bzp7Ri0odq85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalized Term Frequency\n",
        "def termFrequency(term, document):\n",
        "    normalizeDocument = document.lower().split()\n",
        "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
        "\n",
        "# Compute term frequency for a list of documents\n",
        "def compute_tf(docs_list):\n",
        "    for doc in docs_list:\n",
        "        word_list = doc.split()\n",
        "        word_dict = dict.fromkeys(set(word_list), 0)\n",
        "        for word in word_list:\n",
        "            word_dict[word] += 1\n",
        "        df = pd.DataFrame([word_dict])\n",
        "        print(df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "uC0zmdVpdq85"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In reality each document will be of different size. On a large document the frequency of the terms will be much higher than the smaller ones. Hence we need to normalize the document based on its size.\n",
        "* A simple trick is to divide the term frequency by the total number of terms.\n",
        "* For example in Document 1 the term game occurs two times. The total number of terms in the document is 10. Hence the normalized term frequency is 2 / 10 = 0.2.\n",
        "\n",
        "\n",
        "Given below are the normalized term frequency for all the documents."
      ],
      "metadata": {
        "id": "i1s33Q_Xdq85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute normalized term frequency for a list of documents\n",
        "def compute_normalizedtf(documents):\n",
        "    tf_doc = []\n",
        "    for txt in documents:\n",
        "        sentence = txt.split()\n",
        "        norm_tf = dict.fromkeys(set(sentence), 0)\n",
        "        for word in sentence:\n",
        "            norm_tf[word] = termFrequency(word, txt)\n",
        "        tf_doc.append(norm_tf)\n",
        "    return tf_doc\n",
        "\n",
        "tf_doc = compute_normalizedtf(documents)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6KMTOhAGdq86"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Inverse Document Frequency (IDF)\n",
        "\n",
        "* The main purpose of doing a search is to find out relevant documents matching the query.\n",
        "* In Term Frequecy all terms are considered equally important. In fact certain terms that occur too frequently have little power in determining the relevance.\n",
        "* We need a way to weigh down the effects of too frequently occurring terms. Also the terms that occur less in the document can be more relevant.\n",
        "* We need a way to weigh up the effects of less frequently occurring terms. Logarithms helps us to solve this problem.Logarithms helps us to solve this problem.\n",
        "\n",
        "\n",
        "Let us compute IDF for the term start\n",
        "\n",
        "IDF(start) = 1 + loge(Total Number Of Documents / Number Of Documents with term start in it)\n",
        "\n",
        "There are 3 documents in all = Document1, Document2, Document3\n",
        "The term start appears in Document1\n",
        "\n",
        " IDF(start) = 1 + loge(3 / 1)\n",
        "            = 1 + 1.098726209\n",
        "            = 2.098726209"
      ],
      "metadata": {
        "id": "qrXgQfEedq86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute inverse document frequency for all documents\n",
        "def inverseDocumentFrequency(term, allDocuments):\n",
        "    numDocumentsWithThisTerm = 0\n",
        "    for doc in allDocuments:\n",
        "        if term.lower() in doc.lower().split():\n",
        "            numDocumentsWithThisTerm += 1\n",
        "    if numDocumentsWithThisTerm > 0:\n",
        "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "def compute_idf(documents):\n",
        "    idf_dict = {}\n",
        "    for doc in documents:\n",
        "        sentence = doc.split()\n",
        "        for word in sentence:\n",
        "            if word not in idf_dict:  # Check to avoid recomputation\n",
        "                idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
        "    return idf_dict\n",
        "\n",
        "idf_dict = compute_idf(documents)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pjB4Cdetdq86"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.TF * IDF"
      ],
      "metadata": {
        "id": "ivGXXFpWdq87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember we are trying to find out relevant documents for the query: **life learning**\n",
        "\n",
        "* For each term in the query multiply its normalized term frequency with its IDF on each document.\n",
        "* In Document1 for the term life the normalized term frequency is 0.1 and its IDF is 1.405465108.\n",
        "* Multiplying them together we get 0.140550715 (0.1 * 1.405465108).\n",
        "*\n",
        "Given below is TF * IDF calculations for life and learning in all the documents."
      ],
      "metadata": {
        "id": "kgMT4p_Pdq87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf-idf score across all docs for the query string(\"life learning\")\n",
        "def compute_tfidf_with_alldocs(documents, query):\n",
        "    tf_idf = []\n",
        "    query_tokens = query.split()\n",
        "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
        "    for index, doc in enumerate(documents):\n",
        "        df.loc[index, 'doc'] = index\n",
        "        doc_num = tf_doc[index]\n",
        "        sentence = doc.split()\n",
        "        for word in sentence:\n",
        "            if word in query_tokens:\n",
        "                tf_idf_score = doc_num[word] * idf_dict[word]\n",
        "                tf_idf.append(tf_idf_score)\n",
        "                df.loc[index, word] = tf_idf_score\n",
        "    df.fillna(0, inplace=True)\n",
        "    return tf_idf, df\n",
        "\n",
        "tf_idf, df = compute_tfidf_with_alldocs(documents, query)\n",
        "print(df.head())"
      ],
      "metadata": {
        "trusted": true,
        "id": "1KzaIGA-dq88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c674f56-1f93-4893-f03b-60c7061d7047"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   doc      life  learning\n",
            "0    0  0.000000  0.000000\n",
            "1    1  0.002088  0.001348\n",
            "2    2  0.010607  0.000000\n",
            "3    3  0.000000  0.000000\n",
            "4    4  0.000000  0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Vector Space Models and Representation  â€“ Cosine Similarity\n",
        "\n",
        "The set of documents in a collection then is viewed as a set of vectors in a vector space. Each term will have its own axis. Using the formula given below we can find out the similarity between any two documents.\n",
        "\n",
        "* > Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
        "* > Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * â€¦ * d1[n] * d2[n]\n",
        "* > ||d1|| = square root(d1[0]2 + d1[1]2 + ... + d1[n]2)\n",
        "* > ||d2|| = square root(d2[0]2 + d2[1]2 + ... + d2[n]2)\n"
      ],
      "metadata": {
        "id": "NPqhWbZRdq88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from IPython.display import Image\n",
        "#Image(\"../input/tfidf-kernel/cosinesimilarity.jpg\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "yI8Bc1sodq88"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Vectors deals only with numbers. In this example we are dealing with text documents. This was the reason why we used TF and IDF to convert text into numbers so that it can be represented by a vecto\n",
        "\n",
        "\n",
        "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query"
      ],
      "metadata": {
        "id": "ijJpd-0Ydq88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute term frequency for the query string\n",
        "def compute_query_tf(query):\n",
        "    query_norm_tf = {}\n",
        "    tokens = query.split()\n",
        "    for word in tokens:\n",
        "        query_norm_tf[word] = termFrequency(word, query)\n",
        "    return query_norm_tf\n",
        "\n",
        "query_norm_tf = compute_query_tf(query1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "epJentHddq89"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf score for the query string(\"life learning\")\n",
        "def compute_query_tfidf(query):\n",
        "    tfidf_dict_qry = {}\n",
        "    sentence = query.split()\n",
        "    for word in sentence:\n",
        "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
        "    return tfidf_dict_qry\n",
        "tfidf_dict_qry = compute_query_tfidf(query1)\n",
        "print(tfidf_dict_qry)"
      ],
      "metadata": {
        "trusted": true,
        "id": "YhMRb8Lvdq89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4d1f45-f203-4143-fb67-685824c4fa8f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'life': 1.0108256237659905, 'learning': 1.3047189562170503}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now calculate the cosine similarity of the query and Document1.\n",
        "\n",
        "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
        "\n",
        "Dot product(Query, Document1)\n",
        "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
        "     = 0.197545035151\n",
        "\n",
        "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
        "\n",
        "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
        "\n",
        "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
        "                                        = 0.197545035151 / 0.197545035151\n",
        "                                        = 1"
      ],
      "metadata": {
        "id": "1VrzRL0Zdq89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections.abc import Iterable\n",
        "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
        "\n",
        "def cosine_similarity(tfidf_dict_qry, df, query, doc_num):\n",
        "    \"\"\"\n",
        "    Calculate the cosine similarity between a query vector and a document vector represented in TF-IDF scores.\n",
        "\n",
        "    Parameters:\n",
        "    - tfidf_dict_qry: Dictionary of TF-IDF scores for the query.\n",
        "    - df: DataFrame containing TF-IDF scores for all documents.\n",
        "    - query: The query string.\n",
        "    - doc_num: The index of the document to compare with the query.\n",
        "\n",
        "    Returns:\n",
        "    - cos_sim: Cosine similarity score.\n",
        "    \"\"\"\n",
        "    dot_product = 0\n",
        "    qry_mod = 0\n",
        "    doc_mod = 0\n",
        "    tokens = query.split()\n",
        "\n",
        "    # Calculate dot product and vector magnitudes\n",
        "    for keyword in tokens:\n",
        "        if keyword in tfidf_dict_qry and keyword in df.columns:\n",
        "            # Ensure the keyword exists in the DataFrame and query dictionary\n",
        "            query_tfidf = tfidf_dict_qry.get(keyword, 0)\n",
        "            doc_tfidf = df.loc[df['doc'] == doc_num, keyword].values[0] if keyword in df.columns else 0\n",
        "\n",
        "            dot_product += query_tfidf * doc_tfidf\n",
        "            qry_mod += query_tfidf ** 2\n",
        "            doc_mod += doc_tfidf ** 2\n",
        "\n",
        "    qry_mod = np.sqrt(qry_mod)\n",
        "    doc_mod = np.sqrt(doc_mod)\n",
        "    denominator = qry_mod * doc_mod\n",
        "\n",
        "    cos_sim = dot_product / denominator if denominator != 0 else 0\n",
        "    return cos_sim\n",
        "\n",
        "def flatten(lis):\n",
        "    for item in lis:\n",
        "        if isinstance(item, Iterable):\n",
        "            for x in flatten(item):\n",
        "                yield x\n",
        "        else:\n",
        "            yield item"
      ],
      "metadata": {
        "trusted": true,
        "id": "J8Y6qZcGdq89"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_similarity_docs(df, query_tf, query):\n",
        "    cos_sim = []\n",
        "    for doc_num in df.index:\n",
        "        cos_sim.append(cosine_similarity(query_tf, df, query, doc_num))\n",
        "    return cos_sim\n",
        "\n",
        "similarity_scores = rank_similarity_docs(df, tfidf_dict_qry, query)\n",
        "\n",
        "# Print the similarity scores\n",
        "for doc_index, score in enumerate(similarity_scores):\n",
        "    print(f\"Document {doc_index + 1}: Cosine Similarity = {score:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "1nHLJMuSdq8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddcecd1-a802-44b2-b2a7-506553dd6b2a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1: Cosine Similarity = 0.0000\n",
            "Document 2: Cosine Similarity = 0.9432\n",
            "Document 3: Cosine Similarity = 0.6124\n",
            "Document 4: Cosine Similarity = 0.0000\n",
            "Document 5: Cosine Similarity = 0.0000\n",
            "Document 6: Cosine Similarity = 0.7906\n",
            "Document 7: Cosine Similarity = 0.7905\n",
            "Document 8: Cosine Similarity = 0.6124\n",
            "Document 9: Cosine Similarity = 0.0000\n",
            "Document 10: Cosine Similarity = 0.7905\n",
            "Document 11: Cosine Similarity = 0.0000\n",
            "Document 12: Cosine Similarity = 1.0000\n",
            "Document 13: Cosine Similarity = 0.0000\n",
            "Document 14: Cosine Similarity = 0.6124\n",
            "Document 15: Cosine Similarity = 0.0000\n",
            "Document 16: Cosine Similarity = 0.0000\n",
            "Document 17: Cosine Similarity = 0.0000\n",
            "Document 18: Cosine Similarity = 0.6124\n",
            "Document 19: Cosine Similarity = 0.6124\n",
            "Document 20: Cosine Similarity = 0.6124\n",
            "Document 21: Cosine Similarity = 0.0000\n",
            "Document 22: Cosine Similarity = 0.0000\n",
            "Document 23: Cosine Similarity = 0.0000\n",
            "Document 24: Cosine Similarity = 0.0000\n",
            "Document 25: Cosine Similarity = 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* I plotted vector values for the query and documents in 2-dimensional space of life and learning. Document1 has the highest score of 1. This is not surprising as it has both the terms life and learning."
      ],
      "metadata": {
        "id": "iVIU9ar4dq8-"
      }
    }
  ]
}